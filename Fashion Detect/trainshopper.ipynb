{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-contrib-python\n",
      "  Downloading opencv_contrib_python-4.2.0.34-cp37-cp37m-win_amd64.whl (39.5 MB)\n",
      "Requirement already satisfied: numpy>=1.14.5 in c:\\users\\aakas\\anaconda3\\lib\\site-packages (from opencv-contrib-python) (1.18.1)\n",
      "Installing collected packages: opencv-contrib-python\n",
      "Successfully installed opencv-contrib-python-4.2.0.34\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install opencv-contrib-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "# import the necessary packages\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from imutils import paths\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallerVGGNet:\n",
    "    @staticmethod\n",
    "    def build(width, height, depth, classes):\n",
    "        # initialize the model along with the input shape to be \"channels last\" and the channels dimension itself\n",
    "        model = Sequential()\n",
    "        inputShape = (height, width, depth)\n",
    "        chanDim = -1\n",
    "        # if we are using \"channels first\", update the input shape and channels dimension\n",
    "        if K.image_data_format() == \"channels_first\":\n",
    "            inputShape = (depth, height, width)\n",
    "            chanDim = 1\n",
    "        # CONV => RELU => POOL\n",
    "        x = 32\n",
    "        model.add(Conv2D(x, (3, 3), padding=\"same\", input_shape=inputShape))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization(axis=chanDim))\n",
    "        model.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "        model.add(Dropout(0.2))\n",
    "        # (CONV => RELU) * 2 => POOL\n",
    "        model.add(Conv2D(x*2, (3, 3), padding=\"same\"))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization(axis=chanDim))\n",
    "        model.add(Conv2D(x*2, (3, 3), padding=\"same\"))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization(axis=chanDim))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Dropout(0.2))\n",
    "        # (CONV => RELU) * 2 => POOL\n",
    "        model.add(Conv2D(x*4, (3, 3), padding=\"same\"))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization(axis=chanDim))\n",
    "        model.add(Conv2D(x*4, (3, 3), padding=\"same\"))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization(axis=chanDim))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Conv2D(x*8, (3, 3), padding=\"same\"))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization(axis=chanDim))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Dropout(0.2))\n",
    "        \n",
    "        # FC => RELU \n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(2048))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.45))\n",
    "        # softmax classifier\n",
    "        model.add(Dense(classes))\n",
    "        model.add(Activation(\"softmax\"))\n",
    "        # return the constructed network architecture\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading images...\n"
     ]
    }
   ],
   "source": [
    "args = {\"dataset\": \"C:\\\\Users\\\\aakas\\\\CodeStorm\\\\dataset\" } #Come back later for more \n",
    "\n",
    "# initialize the number of epochs to train for, initial learning rate, batch size, and image dimensions\n",
    "EPOCHS = 100\n",
    "INIT_LR = 3e-3\n",
    "BS = 32\n",
    "IMAGE_DIMS = (96, 96, 3)\n",
    "# initialize the data and labels\n",
    "data = []\n",
    "labels = []\n",
    "# grab the image paths and randomly shuffle them\n",
    "print(\"[INFO] loading images...\")\n",
    "imagePaths = sorted(list(paths.list_images(args[\"dataset\"])))\n",
    "random.seed(21)\n",
    "random.shuffle(imagePaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over the input images\n",
    "for imagePath in imagePaths:\n",
    "    # load the image, pre-process it, and store it in the data list\n",
    "    image = cv2.imread(imagePath)\n",
    "    image = cv2.resize(image, (IMAGE_DIMS[1], IMAGE_DIMS[0]))\n",
    "    image = img_to_array(image)\n",
    "    data.append(image)\n",
    "    # extract the class label from the image path and update the labels list\n",
    "    label = imagePath.split(os.path.sep)[-2]\n",
    "    labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] data matrix: 548.86MB\n"
     ]
    }
   ],
   "source": [
    "# scale the raw pixel intensities to the range [0, 1]\n",
    "data = np.array(data, dtype=\"float\") / 255.0\n",
    "labels = np.array(labels)\n",
    "print(\"[INFO] data matrix: {:.2f}MB\".format(data.nbytes / (1024 * 1000.0)))\n",
    "# binarize the labels\n",
    "lb = LabelBinarizer()\n",
    "labels = lb.fit_transform(labels)\n",
    "# partition the data into training and testing splits using 80% of\n",
    "# the data for training and the remaining 20% for testing\n",
    "(trainX, testX, trainY, testY) = train_test_split(data,labels, test_size=0.2, random_state=21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constructing the image generator for data augmentation\n",
    "aug = ImageDataGenerator(rotation_range=20, width_shift_range=0.3,height_shift_range=0.3, shear_range=0.25, zoom_range=0.25, \n",
    "                         horizontal_flip=True, fill_mode=\"nearest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] compiling model...\n",
      "[INFO] training network...\n",
      "Epoch 1/100\n",
      "63/63 [==============================] - 69s 1s/step - loss: 3.4459 - accuracy: 0.2035 - val_loss: 4.6384 - val_accuracy: 0.1198 - lr: 0.0030\n",
      "Epoch 2/100\n",
      "63/63 [==============================] - 63s 995ms/step - loss: 3.1126 - accuracy: 0.2365 - val_loss: 3.1970 - val_accuracy: 0.2083 - lr: 0.0030\n",
      "Epoch 3/100\n",
      "63/63 [==============================] - 61s 976ms/step - loss: 2.9555 - accuracy: 0.2703 - val_loss: 2.9591 - val_accuracy: 0.2200 - lr: 0.0030\n",
      "Epoch 4/100\n",
      "63/63 [==============================] - 61s 965ms/step - loss: 2.8017 - accuracy: 0.2700 - val_loss: 2.0617 - val_accuracy: 0.3163 - lr: 0.0030\n",
      "Epoch 5/100\n",
      "63/63 [==============================] - 63s 1s/step - loss: 2.6496 - accuracy: 0.2940 - val_loss: 2.4005 - val_accuracy: 0.2711 - lr: 0.0030\n",
      "Epoch 6/100\n",
      "63/63 [==============================] - 62s 992ms/step - loss: 2.5166 - accuracy: 0.2965 - val_loss: 2.1392 - val_accuracy: 0.3163 - lr: 0.0030\n",
      "Epoch 7/100\n",
      "63/63 [==============================] - 64s 1s/step - loss: 2.6824 - accuracy: 0.2865 - val_loss: 2.1506 - val_accuracy: 0.2849 - lr: 0.0030\n",
      "Epoch 8/100\n",
      "63/63 [==============================] - 62s 979ms/step - loss: 2.5918 - accuracy: 0.3060 - val_loss: 3.0364 - val_accuracy: 0.3124 - lr: 0.0030\n",
      "Epoch 9/100\n",
      "63/63 [==============================] - 61s 975ms/step - loss: 2.4943 - accuracy: 0.3145 - val_loss: 2.7542 - val_accuracy: 0.3438 - lr: 0.0030\n",
      "Epoch 10/100\n",
      "63/63 [==============================] - 62s 990ms/step - loss: 2.5711 - accuracy: 0.3150 - val_loss: 3.0511 - val_accuracy: 0.3360 - lr: 0.0030\n",
      "Epoch 11/100\n",
      "63/63 [==============================] - 60s 957ms/step - loss: 2.4583 - accuracy: 0.3405 - val_loss: 3.6285 - val_accuracy: 0.2888 - lr: 0.0030\n",
      "Epoch 12/100\n",
      "63/63 [==============================] - 61s 963ms/step - loss: 2.4265 - accuracy: 0.3265 - val_loss: 7.2064 - val_accuracy: 0.2554 - lr: 0.0030\n",
      "Epoch 13/100\n",
      "63/63 [==============================] - 60s 958ms/step - loss: 2.4819 - accuracy: 0.3415 - val_loss: 3.8221 - val_accuracy: 0.3418 - lr: 0.0030\n",
      "Epoch 14/100\n",
      "63/63 [==============================] - 57s 910ms/step - loss: 2.3899 - accuracy: 0.3420 - val_loss: 1.7318 - val_accuracy: 0.3733 - lr: 0.0030\n",
      "Epoch 15/100\n",
      "63/63 [==============================] - 59s 929ms/step - loss: 2.3293 - accuracy: 0.3490 - val_loss: 1.8953 - val_accuracy: 0.3615 - lr: 0.0030\n",
      "Epoch 16/100\n",
      "63/63 [==============================] - 63s 1s/step - loss: 2.4353 - accuracy: 0.3470 - val_loss: 3.2122 - val_accuracy: 0.3320 - lr: 0.0030\n",
      "Epoch 17/100\n",
      "63/63 [==============================] - 61s 972ms/step - loss: 2.3362 - accuracy: 0.3620 - val_loss: 6.6398 - val_accuracy: 0.2279 - lr: 0.0030\n",
      "Epoch 18/100\n",
      "63/63 [==============================] - 67s 1s/step - loss: 2.3626 - accuracy: 0.3570 - val_loss: 3.1639 - val_accuracy: 0.3988 - lr: 0.0030\n",
      "Epoch 19/100\n",
      "63/63 [==============================] - 66s 1s/step - loss: 2.2767 - accuracy: 0.3665 - val_loss: 1.9255 - val_accuracy: 0.4165 - lr: 0.0030\n",
      "Epoch 20/100\n",
      "63/63 [==============================] - 63s 1s/step - loss: 2.3421 - accuracy: 0.3675 - val_loss: 1.8796 - val_accuracy: 0.3792 - lr: 0.0030\n",
      "Epoch 21/100\n",
      "63/63 [==============================] - 59s 930ms/step - loss: 2.2503 - accuracy: 0.3685 - val_loss: 3.7764 - val_accuracy: 0.3143 - lr: 0.0030\n",
      "Epoch 22/100\n",
      "63/63 [==============================] - 60s 954ms/step - loss: 2.2907 - accuracy: 0.3725 - val_loss: 3.4548 - val_accuracy: 0.4381 - lr: 0.0030\n",
      "Epoch 23/100\n",
      "63/63 [==============================] - 60s 950ms/step - loss: 2.3565 - accuracy: 0.3455 - val_loss: 2.4538 - val_accuracy: 0.3124 - lr: 0.0030\n",
      "Epoch 24/100\n",
      "63/63 [==============================] - ETA: 0s - loss: 2.0931 - accuracy: 0.3810\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.0006000000052154065.\n",
      "63/63 [==============================] - 63s 996ms/step - loss: 2.0931 - accuracy: 0.3810 - val_loss: 1.8652 - val_accuracy: 0.4185 - lr: 0.0030\n",
      "Epoch 25/100\n",
      "63/63 [==============================] - 62s 986ms/step - loss: 1.8940 - accuracy: 0.4190 - val_loss: 1.5666 - val_accuracy: 0.5010 - lr: 6.0000e-04\n",
      "Epoch 26/100\n",
      "63/63 [==============================] - 63s 1s/step - loss: 1.7107 - accuracy: 0.4395 - val_loss: 1.3492 - val_accuracy: 0.5481 - lr: 6.0000e-04\n",
      "Epoch 27/100\n",
      "63/63 [==============================] - 61s 965ms/step - loss: 1.7779 - accuracy: 0.4305 - val_loss: 1.4974 - val_accuracy: 0.5049 - lr: 6.0000e-04\n",
      "Epoch 28/100\n",
      "63/63 [==============================] - 60s 949ms/step - loss: 1.7269 - accuracy: 0.4480 - val_loss: 1.4281 - val_accuracy: 0.5305 - lr: 6.0000e-04\n",
      "Epoch 29/100\n",
      "63/63 [==============================] - 64s 1s/step - loss: 1.6467 - accuracy: 0.4635 - val_loss: 1.7917 - val_accuracy: 0.4538 - lr: 6.0000e-04\n",
      "Epoch 30/100\n",
      "63/63 [==============================] - 58s 924ms/step - loss: 1.7589 - accuracy: 0.4480 - val_loss: 1.9502 - val_accuracy: 0.5305 - lr: 6.0000e-04\n",
      "Epoch 31/100\n",
      "63/63 [==============================] - 66s 1s/step - loss: 1.6412 - accuracy: 0.4585 - val_loss: 1.6309 - val_accuracy: 0.5403 - lr: 6.0000e-04\n",
      "Epoch 32/100\n",
      "63/63 [==============================] - 62s 990ms/step - loss: 1.6807 - accuracy: 0.4670 - val_loss: 1.4672 - val_accuracy: 0.5639 - lr: 6.0000e-04\n",
      "Epoch 33/100\n",
      " 4/63 [>.............................] - ETA: 42s - loss: 1.7586 - accuracy: 0.4375"
     ]
    }
   ],
   "source": [
    "# initialize the model\n",
    "print(\"[INFO] compiling model...\")\n",
    "model = SmallerVGGNet.build(width=IMAGE_DIMS[1], height=IMAGE_DIMS[0],depth=IMAGE_DIMS[2], classes=len(lb.classes_))\n",
    "opt = Adam(lr=INIT_LR, decay=INIT_LR / 100)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "mcp_save = ModelCheckpoint('.mdl_wts.hdf5', save_best_only=True, monitor='val_accuracy', mode='max')\n",
    "reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=10, verbose=1, min_delta=1e-4, mode='min')\n",
    "# train the network\n",
    "print(\"[INFO] training network...\")\n",
    "H = model.fit(\n",
    "    x=aug.flow(trainX, trainY, batch_size=BS),\n",
    "    validation_data=(testX, testY),\n",
    "    callbacks = [mcp_save, reduce_lr_loss],\n",
    "    steps_per_epoch=len(trainX) // BS,\n",
    "    epochs=100, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the training loss and accuracy\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "N = 100\n",
    "plt.plot(np.arange(0, N), H.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(np.arange(0, N), H.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(np.arange(0, N), H.history[\"accuracy\"], label=\"train_acc\")\n",
    "plt.plot(np.arange(0, N), H.history[\"val_accuracy\"], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.savefig(\"plot1.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
